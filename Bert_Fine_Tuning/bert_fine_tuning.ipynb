{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune BERT with my own dataset\n",
    "\n",
    "[Tutorial](https://huggingface.co/blog/how-to-train-sentence-transformers)\n",
    "\n",
    "[Posible data 1](https://www.argentina.gob.ar/desarrollosocial/entrevistasyopinion)\n",
    "\n",
    "[Posible data 2](https://www.reddit.com/r/RepublicaArgentina/)\n",
    "\n",
    "[Posible data 3](https://www.reddit.com/r/Republica_Argentina/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_MyModule = '..'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, path_to_MyModule) \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "\n",
    "from MyModule.GeneralFunctions import *\n",
    "from MyModule.SummarizationFunctions import *\n",
    "from MyModule.SummarizationFunctions import MostRepresentativeDocs\n",
    "from MyModule.SamplingFunctions import *\n",
    "from MyModule.SemanticSimilarityFunctions import *\n",
    "from MyModule.TopicModelingFunctinos import *\n",
    "from MyModule.FineTuningFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('..\\datos.xlsx')[['ID','texto','desafio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza\n",
    "df.drop_duplicates(subset='texto', inplace=True)\n",
    "\n",
    "# Quitando texto de mas en columna \"desafio\"\n",
    "df['desafio'] = df['desafio'].apply(lambda x: re.findall('[0-9]+', x)[0])\n",
    "\n",
    "# A str\n",
    "df['texto'] = df['texto'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_object = Preprocess(lemma=False, stopwords=False)\n",
    "documents = df['texto'].values.tolist()\n",
    "documents = pp_object.preprocess(documents)\n",
    "documents = [doc for doc in documents if doc != '']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_pp.txt', 'r') as f:\n",
    "    documents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_object = Preprocess(lemma=False, stopwords=False)\n",
    "documents = pp_object.preprocess(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"hiiamsid/sentence_similarity_spanish_es\"\n",
    "my_model = MyFineTunedBert()\n",
    "my_model.load_model(\"fine_tuned_bert_1.pt\", model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([11.637869  , -0.58177364, -0.912256  , ..., -0.46812254,\n",
       "        -1.4491892 ,  0.4982587 ], dtype=float32)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.encode(['Soy de derecha y vos no'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "# torch.save(model.state_dict(), \"fine_tuned_bert_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at hiiamsid/sentence_similarity_spanish_es and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_id = \"hiiamsid/sentence_similarity_spanish_es\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "model = BertForMaskedLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset for MLM training\n",
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        if len(tokens) > self.max_len - 2:\n",
    "            tokens = tokens[:self.max_len - 2]\n",
    "        input_ids = self.tokenizer.encode(tokens, add_special_tokens=True)\n",
    "        # Randomly mask out some tokens\n",
    "        masked_ids = input_ids.copy()\n",
    "        for i, token_id in enumerate(masked_ids):\n",
    "            if token_id != self.tokenizer.cls_token_id and token_id != self.tokenizer.sep_token_id:\n",
    "                if torch.rand(1) < 0.15:\n",
    "                    masked_ids[i] = self.tokenizer.mask_token_id\n",
    "        # Create attention masks and segment IDs\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        segment_ids = [0] * len(input_ids)\n",
    "        return masked_ids, attention_mask, segment_ids, input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a collate function to pad the sequences to the same length\n",
    "def collate_fn(batch):\n",
    "    masked_ids = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(item[0]) for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(item[1]) for item in batch], batch_first=True, padding_value=0)\n",
    "    segment_ids = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(item[2]) for item in batch], batch_first=True, padding_value=0)\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(item[3]) for item in batch], batch_first=True, padding_value=0)\n",
    "    return masked_ids, attention_mask, segment_ids, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 1 of 310] this loss: 0.116, total loss: 0.116\n",
      "[Epoch 1, Batch 6 of 310] this loss: 0.361, total loss: 0.227\n",
      "[Epoch 1, Batch 11 of 310] this loss: 0.169, total loss: 0.172\n",
      "[Epoch 1, Batch 16 of 310] this loss: 0.129, total loss: 0.143\n",
      "[Epoch 1, Batch 21 of 310] this loss: 0.146, total loss: 0.130\n",
      "[Epoch 1, Batch 26 of 310] this loss: 0.147, total loss: 0.121\n",
      "[Epoch 1, Batch 31 of 310] this loss: 0.138, total loss: 0.114\n",
      "[Epoch 1, Batch 36 of 310] this loss: 0.129, total loss: 0.108\n",
      "[Epoch 1, Batch 41 of 310] this loss: 0.127, total loss: 0.104\n",
      "[Epoch 1, Batch 46 of 310] this loss: 0.154, total loss: 0.103\n",
      "[Epoch 1, Batch 51 of 310] this loss: 0.124, total loss: 0.100\n",
      "[Epoch 1, Batch 56 of 310] this loss: 0.129, total loss: 0.098\n",
      "[Epoch 1, Batch 61 of 310] this loss: 0.163, total loss: 0.097\n",
      "[Epoch 1, Batch 66 of 310] this loss: 0.165, total loss: 0.098\n",
      "[Epoch 1, Batch 71 of 310] this loss: 0.116, total loss: 0.096\n",
      "[Epoch 1, Batch 76 of 310] this loss: 0.126, total loss: 0.094\n",
      "[Epoch 1, Batch 81 of 310] this loss: 0.167, total loss: 0.095\n",
      "[Epoch 1, Batch 86 of 310] this loss: 0.141, total loss: 0.094\n",
      "[Epoch 1, Batch 91 of 310] this loss: 0.139, total loss: 0.093\n",
      "[Epoch 1, Batch 96 of 310] this loss: 0.143, total loss: 0.093\n",
      "[Epoch 1, Batch 101 of 310] this loss: 0.139, total loss: 0.092\n",
      "[Epoch 1, Batch 106 of 310] this loss: 0.159, total loss: 0.092\n",
      "[Epoch 1, Batch 111 of 310] this loss: 0.128, total loss: 0.092\n",
      "[Epoch 1, Batch 116 of 310] this loss: 0.120, total loss: 0.091\n",
      "[Epoch 1, Batch 121 of 310] this loss: 0.122, total loss: 0.090\n",
      "[Epoch 1, Batch 126 of 310] this loss: 0.156, total loss: 0.090\n",
      "[Epoch 1, Batch 131 of 310] this loss: 0.132, total loss: 0.090\n",
      "[Epoch 1, Batch 136 of 310] this loss: 0.144, total loss: 0.089\n",
      "[Epoch 1, Batch 141 of 310] this loss: 0.181, total loss: 0.090\n",
      "[Epoch 1, Batch 146 of 310] this loss: 0.127, total loss: 0.089\n",
      "[Epoch 1, Batch 151 of 310] this loss: 0.153, total loss: 0.089\n",
      "[Epoch 1, Batch 156 of 310] this loss: 0.181, total loss: 0.090\n",
      "[Epoch 1, Batch 161 of 310] this loss: 0.123, total loss: 0.090\n",
      "[Epoch 1, Batch 166 of 310] this loss: 0.136, total loss: 0.090\n",
      "[Epoch 1, Batch 171 of 310] this loss: 0.149, total loss: 0.090\n",
      "[Epoch 1, Batch 176 of 310] this loss: 0.165, total loss: 0.090\n",
      "[Epoch 1, Batch 181 of 310] this loss: 0.129, total loss: 0.090\n",
      "[Epoch 1, Batch 186 of 310] this loss: 0.120, total loss: 0.089\n",
      "[Epoch 1, Batch 191 of 310] this loss: 0.137, total loss: 0.089\n",
      "[Epoch 1, Batch 196 of 310] this loss: 0.100, total loss: 0.088\n",
      "[Epoch 1, Batch 201 of 310] this loss: 0.126, total loss: 0.088\n",
      "[Epoch 1, Batch 206 of 310] this loss: 0.143, total loss: 0.088\n",
      "[Epoch 1, Batch 211 of 310] this loss: 0.143, total loss: 0.088\n",
      "[Epoch 1, Batch 216 of 310] this loss: 0.121, total loss: 0.087\n",
      "[Epoch 1, Batch 221 of 310] this loss: 0.144, total loss: 0.087\n",
      "[Epoch 1, Batch 226 of 310] this loss: 0.140, total loss: 0.087\n",
      "[Epoch 1, Batch 231 of 310] this loss: 0.153, total loss: 0.087\n",
      "[Epoch 1, Batch 236 of 310] this loss: 0.151, total loss: 0.087\n",
      "[Epoch 1, Batch 241 of 310] this loss: 0.147, total loss: 0.087\n",
      "[Epoch 1, Batch 246 of 310] this loss: 0.141, total loss: 0.087\n",
      "[Epoch 1, Batch 251 of 310] this loss: 0.147, total loss: 0.087\n",
      "[Epoch 1, Batch 256 of 310] this loss: 0.126, total loss: 0.087\n",
      "[Epoch 1, Batch 261 of 310] this loss: 0.152, total loss: 0.087\n",
      "[Epoch 1, Batch 266 of 310] this loss: 0.151, total loss: 0.087\n",
      "[Epoch 1, Batch 271 of 310] this loss: 0.134, total loss: 0.087\n",
      "[Epoch 1, Batch 276 of 310] this loss: 0.113, total loss: 0.087\n",
      "[Epoch 1, Batch 281 of 310] this loss: 0.133, total loss: 0.087\n",
      "[Epoch 1, Batch 286 of 310] this loss: 0.150, total loss: 0.087\n",
      "[Epoch 1, Batch 291 of 310] this loss: 0.135, total loss: 0.087\n",
      "[Epoch 1, Batch 296 of 310] this loss: 0.131, total loss: 0.087\n",
      "[Epoch 1, Batch 301 of 310] this loss: 0.179, total loss: 0.087\n",
      "[Epoch 1, Batch 306 of 310] this loss: 0.155, total loss: 0.087\n",
      "[Epoch 2, Batch 1 of 310] this loss: 0.036, total loss: 0.087\n",
      "[Epoch 2, Batch 6 of 310] this loss: 0.148, total loss: 0.087\n",
      "[Epoch 2, Batch 11 of 310] this loss: 0.155, total loss: 0.087\n",
      "[Epoch 2, Batch 16 of 310] this loss: 0.119, total loss: 0.087\n",
      "[Epoch 2, Batch 21 of 310] this loss: 0.127, total loss: 0.086\n",
      "[Epoch 2, Batch 26 of 310] this loss: 0.177, total loss: 0.087\n",
      "[Epoch 2, Batch 31 of 310] this loss: 0.133, total loss: 0.087\n",
      "[Epoch 2, Batch 36 of 310] this loss: 0.141, total loss: 0.087\n",
      "[Epoch 2, Batch 41 of 310] this loss: 0.161, total loss: 0.087\n",
      "[Epoch 2, Batch 46 of 310] this loss: 0.142, total loss: 0.087\n",
      "[Epoch 2, Batch 51 of 310] this loss: 0.169, total loss: 0.087\n",
      "[Epoch 2, Batch 56 of 310] this loss: 0.126, total loss: 0.087\n",
      "[Epoch 2, Batch 61 of 310] this loss: 0.123, total loss: 0.087\n",
      "[Epoch 2, Batch 66 of 310] this loss: 0.156, total loss: 0.087\n",
      "[Epoch 2, Batch 71 of 310] this loss: 0.134, total loss: 0.087\n",
      "[Epoch 2, Batch 76 of 310] this loss: 0.137, total loss: 0.087\n",
      "[Epoch 2, Batch 81 of 310] this loss: 0.134, total loss: 0.087\n",
      "[Epoch 2, Batch 86 of 310] this loss: 0.137, total loss: 0.087\n",
      "[Epoch 2, Batch 91 of 310] this loss: 0.116, total loss: 0.086\n",
      "[Epoch 2, Batch 96 of 310] this loss: 0.143, total loss: 0.086\n",
      "[Epoch 2, Batch 101 of 310] this loss: 0.150, total loss: 0.086\n",
      "[Epoch 2, Batch 106 of 310] this loss: 0.174, total loss: 0.087\n",
      "[Epoch 2, Batch 111 of 310] this loss: 0.148, total loss: 0.087\n",
      "[Epoch 2, Batch 116 of 310] this loss: 0.144, total loss: 0.087\n",
      "[Epoch 2, Batch 121 of 310] this loss: 0.147, total loss: 0.086\n",
      "[Epoch 2, Batch 126 of 310] this loss: 0.133, total loss: 0.086\n",
      "[Epoch 2, Batch 131 of 310] this loss: 0.119, total loss: 0.086\n",
      "[Epoch 2, Batch 136 of 310] this loss: 0.144, total loss: 0.086\n",
      "[Epoch 2, Batch 141 of 310] this loss: 0.155, total loss: 0.086\n",
      "[Epoch 2, Batch 146 of 310] this loss: 0.155, total loss: 0.086\n",
      "[Epoch 2, Batch 151 of 310] this loss: 0.143, total loss: 0.086\n",
      "[Epoch 2, Batch 156 of 310] this loss: 0.149, total loss: 0.086\n",
      "[Epoch 2, Batch 161 of 310] this loss: 0.122, total loss: 0.086\n",
      "[Epoch 2, Batch 166 of 310] this loss: 0.144, total loss: 0.086\n",
      "[Epoch 2, Batch 171 of 310] this loss: 0.119, total loss: 0.086\n",
      "[Epoch 2, Batch 176 of 310] this loss: 0.127, total loss: 0.086\n",
      "[Epoch 2, Batch 181 of 310] this loss: 0.132, total loss: 0.086\n",
      "[Epoch 2, Batch 186 of 310] this loss: 0.133, total loss: 0.086\n",
      "[Epoch 2, Batch 191 of 310] this loss: 0.157, total loss: 0.086\n",
      "[Epoch 2, Batch 196 of 310] this loss: 0.162, total loss: 0.086\n",
      "[Epoch 2, Batch 201 of 310] this loss: 0.134, total loss: 0.086\n",
      "[Epoch 2, Batch 206 of 310] this loss: 0.162, total loss: 0.086\n",
      "[Epoch 2, Batch 211 of 310] this loss: 0.113, total loss: 0.086\n",
      "[Epoch 2, Batch 216 of 310] this loss: 0.148, total loss: 0.086\n",
      "[Epoch 2, Batch 221 of 310] this loss: 0.108, total loss: 0.086\n",
      "[Epoch 2, Batch 226 of 310] this loss: 0.127, total loss: 0.086\n",
      "[Epoch 2, Batch 231 of 310] this loss: 0.121, total loss: 0.085\n",
      "[Epoch 2, Batch 236 of 310] this loss: 0.149, total loss: 0.085\n",
      "[Epoch 2, Batch 241 of 310] this loss: 0.120, total loss: 0.086\n",
      "[Epoch 2, Batch 246 of 310] this loss: 0.141, total loss: 0.086\n",
      "[Epoch 2, Batch 251 of 310] this loss: 0.134, total loss: 0.086\n",
      "[Epoch 2, Batch 256 of 310] this loss: 0.155, total loss: 0.086\n",
      "[Epoch 2, Batch 261 of 310] this loss: 0.145, total loss: 0.086\n",
      "[Epoch 2, Batch 266 of 310] this loss: 0.170, total loss: 0.086\n",
      "[Epoch 2, Batch 271 of 310] this loss: 0.144, total loss: 0.086\n",
      "[Epoch 2, Batch 276 of 310] this loss: 0.158, total loss: 0.086\n",
      "[Epoch 2, Batch 281 of 310] this loss: 0.140, total loss: 0.086\n",
      "[Epoch 2, Batch 286 of 310] this loss: 0.129, total loss: 0.086\n",
      "[Epoch 2, Batch 291 of 310] this loss: 0.167, total loss: 0.086\n",
      "[Epoch 2, Batch 296 of 310] this loss: 0.141, total loss: 0.086\n",
      "[Epoch 2, Batch 301 of 310] this loss: 0.156, total loss: 0.086\n",
      "[Epoch 2, Batch 306 of 310] this loss: 0.167, total loss: 0.086\n",
      "[Epoch 3, Batch 1 of 310] this loss: 0.031, total loss: 0.086\n",
      "[Epoch 3, Batch 6 of 310] this loss: 0.160, total loss: 0.086\n",
      "[Epoch 3, Batch 11 of 310] this loss: 0.144, total loss: 0.086\n",
      "[Epoch 3, Batch 16 of 310] this loss: 0.117, total loss: 0.086\n",
      "[Epoch 3, Batch 21 of 310] this loss: 0.137, total loss: 0.086\n",
      "[Epoch 3, Batch 26 of 310] this loss: 0.144, total loss: 0.086\n",
      "[Epoch 3, Batch 31 of 310] this loss: 0.153, total loss: 0.086\n",
      "[Epoch 3, Batch 36 of 310] this loss: 0.151, total loss: 0.086\n",
      "[Epoch 3, Batch 41 of 310] this loss: 0.141, total loss: 0.086\n",
      "[Epoch 3, Batch 46 of 310] this loss: 0.150, total loss: 0.086\n",
      "[Epoch 3, Batch 51 of 310] this loss: 0.137, total loss: 0.086\n",
      "[Epoch 3, Batch 56 of 310] this loss: 0.160, total loss: 0.086\n",
      "[Epoch 3, Batch 61 of 310] this loss: 0.144, total loss: 0.086\n",
      "[Epoch 3, Batch 66 of 310] this loss: 0.152, total loss: 0.086\n",
      "[Epoch 3, Batch 71 of 310] this loss: 0.133, total loss: 0.086\n",
      "[Epoch 3, Batch 76 of 310] this loss: 0.122, total loss: 0.086\n",
      "[Epoch 3, Batch 81 of 310] this loss: 0.143, total loss: 0.086\n",
      "[Epoch 3, Batch 86 of 310] this loss: 0.177, total loss: 0.086\n",
      "[Epoch 3, Batch 91 of 310] this loss: 0.102, total loss: 0.086\n",
      "[Epoch 3, Batch 96 of 310] this loss: 0.165, total loss: 0.086\n",
      "[Epoch 3, Batch 101 of 310] this loss: 0.148, total loss: 0.086\n",
      "[Epoch 3, Batch 106 of 310] this loss: 0.129, total loss: 0.086\n",
      "[Epoch 3, Batch 111 of 310] this loss: 0.121, total loss: 0.086\n",
      "[Epoch 3, Batch 116 of 310] this loss: 0.140, total loss: 0.086\n",
      "[Epoch 3, Batch 121 of 310] this loss: 0.158, total loss: 0.086\n",
      "[Epoch 3, Batch 126 of 310] this loss: 0.137, total loss: 0.086\n",
      "[Epoch 3, Batch 131 of 310] this loss: 0.169, total loss: 0.086\n",
      "[Epoch 3, Batch 136 of 310] this loss: 0.133, total loss: 0.086\n",
      "[Epoch 3, Batch 141 of 310] this loss: 0.154, total loss: 0.086\n",
      "[Epoch 3, Batch 146 of 310] this loss: 0.137, total loss: 0.086\n",
      "[Epoch 3, Batch 151 of 310] this loss: 0.159, total loss: 0.086\n",
      "[Epoch 3, Batch 156 of 310] this loss: 0.150, total loss: 0.086\n",
      "[Epoch 3, Batch 161 of 310] this loss: 0.164, total loss: 0.086\n",
      "[Epoch 3, Batch 166 of 310] this loss: 0.143, total loss: 0.086\n",
      "[Epoch 3, Batch 171 of 310] this loss: 0.141, total loss: 0.086\n",
      "[Epoch 3, Batch 176 of 310] this loss: 0.147, total loss: 0.086\n",
      "[Epoch 3, Batch 181 of 310] this loss: 0.120, total loss: 0.086\n",
      "[Epoch 3, Batch 186 of 310] this loss: 0.123, total loss: 0.086\n",
      "[Epoch 3, Batch 191 of 310] this loss: 0.128, total loss: 0.086\n",
      "[Epoch 3, Batch 196 of 310] this loss: 0.120, total loss: 0.086\n",
      "[Epoch 3, Batch 201 of 310] this loss: 0.128, total loss: 0.086\n",
      "[Epoch 3, Batch 206 of 310] this loss: 0.123, total loss: 0.086\n",
      "[Epoch 3, Batch 211 of 310] this loss: 0.123, total loss: 0.086\n",
      "[Epoch 3, Batch 216 of 310] this loss: 0.132, total loss: 0.086\n",
      "[Epoch 3, Batch 221 of 310] this loss: 0.137, total loss: 0.086\n",
      "[Epoch 3, Batch 226 of 310] this loss: 0.143, total loss: 0.086\n",
      "[Epoch 3, Batch 231 of 310] this loss: 0.155, total loss: 0.086\n",
      "[Epoch 3, Batch 236 of 310] this loss: 0.140, total loss: 0.086\n",
      "[Epoch 3, Batch 241 of 310] this loss: 0.140, total loss: 0.086\n",
      "[Epoch 3, Batch 246 of 310] this loss: 0.153, total loss: 0.086\n",
      "[Epoch 3, Batch 251 of 310] this loss: 0.137, total loss: 0.086\n",
      "[Epoch 3, Batch 256 of 310] this loss: 0.118, total loss: 0.086\n",
      "[Epoch 3, Batch 261 of 310] this loss: 0.128, total loss: 0.086\n",
      "[Epoch 3, Batch 266 of 310] this loss: 0.127, total loss: 0.086\n",
      "[Epoch 3, Batch 271 of 310] this loss: 0.165, total loss: 0.086\n",
      "[Epoch 3, Batch 276 of 310] this loss: 0.134, total loss: 0.086\n",
      "[Epoch 3, Batch 281 of 310] this loss: 0.139, total loss: 0.086\n",
      "[Epoch 3, Batch 286 of 310] this loss: 0.154, total loss: 0.086\n",
      "[Epoch 3, Batch 291 of 310] this loss: 0.131, total loss: 0.086\n",
      "[Epoch 3, Batch 296 of 310] this loss: 0.146, total loss: 0.086\n",
      "[Epoch 3, Batch 301 of 310] this loss: 0.156, total loss: 0.086\n",
      "[Epoch 3, Batch 306 of 310] this loss: 0.181, total loss: 0.086\n",
      "[Epoch 4, Batch 1 of 310] this loss: 0.024, total loss: 0.086\n",
      "[Epoch 4, Batch 6 of 310] this loss: 0.147, total loss: 0.086\n",
      "[Epoch 4, Batch 11 of 310] this loss: 0.150, total loss: 0.086\n",
      "[Epoch 4, Batch 16 of 310] this loss: 0.126, total loss: 0.086\n",
      "[Epoch 4, Batch 21 of 310] this loss: 0.147, total loss: 0.086\n",
      "[Epoch 4, Batch 26 of 310] this loss: 0.145, total loss: 0.086\n",
      "[Epoch 4, Batch 31 of 310] this loss: 0.160, total loss: 0.086\n",
      "[Epoch 4, Batch 36 of 310] this loss: 0.175, total loss: 0.086\n",
      "[Epoch 4, Batch 41 of 310] this loss: 0.145, total loss: 0.086\n",
      "[Epoch 4, Batch 46 of 310] this loss: 0.136, total loss: 0.086\n",
      "[Epoch 4, Batch 51 of 310] this loss: 0.154, total loss: 0.086\n",
      "[Epoch 4, Batch 56 of 310] this loss: 0.133, total loss: 0.086\n",
      "[Epoch 4, Batch 61 of 310] this loss: 0.123, total loss: 0.086\n",
      "[Epoch 4, Batch 66 of 310] this loss: 0.143, total loss: 0.086\n",
      "[Epoch 4, Batch 71 of 310] this loss: 0.152, total loss: 0.086\n",
      "[Epoch 4, Batch 76 of 310] this loss: 0.133, total loss: 0.086\n",
      "[Epoch 4, Batch 81 of 310] this loss: 0.126, total loss: 0.086\n",
      "[Epoch 4, Batch 86 of 310] this loss: 0.132, total loss: 0.086\n",
      "[Epoch 4, Batch 91 of 310] this loss: 0.127, total loss: 0.086\n",
      "[Epoch 4, Batch 96 of 310] this loss: 0.125, total loss: 0.086\n",
      "[Epoch 4, Batch 101 of 310] this loss: 0.125, total loss: 0.086\n",
      "[Epoch 4, Batch 106 of 310] this loss: 0.149, total loss: 0.086\n",
      "[Epoch 4, Batch 111 of 310] this loss: 0.154, total loss: 0.086\n",
      "[Epoch 4, Batch 116 of 310] this loss: 0.152, total loss: 0.086\n",
      "[Epoch 4, Batch 121 of 310] this loss: 0.138, total loss: 0.086\n",
      "[Epoch 4, Batch 126 of 310] this loss: 0.130, total loss: 0.086\n",
      "[Epoch 4, Batch 131 of 310] this loss: 0.139, total loss: 0.086\n",
      "[Epoch 4, Batch 136 of 310] this loss: 0.132, total loss: 0.086\n",
      "[Epoch 4, Batch 141 of 310] this loss: 0.153, total loss: 0.086\n",
      "[Epoch 4, Batch 146 of 310] this loss: 0.128, total loss: 0.086\n",
      "[Epoch 4, Batch 151 of 310] this loss: 0.134, total loss: 0.086\n",
      "[Epoch 4, Batch 156 of 310] this loss: 0.136, total loss: 0.086\n",
      "[Epoch 4, Batch 161 of 310] this loss: 0.153, total loss: 0.086\n",
      "[Epoch 4, Batch 166 of 310] this loss: 0.138, total loss: 0.086\n",
      "[Epoch 4, Batch 171 of 310] this loss: 0.155, total loss: 0.086\n",
      "[Epoch 4, Batch 176 of 310] this loss: 0.125, total loss: 0.086\n",
      "[Epoch 4, Batch 181 of 310] this loss: 0.133, total loss: 0.086\n",
      "[Epoch 4, Batch 186 of 310] this loss: 0.102, total loss: 0.085\n",
      "[Epoch 4, Batch 191 of 310] this loss: 0.160, total loss: 0.086\n",
      "[Epoch 4, Batch 196 of 310] this loss: 0.156, total loss: 0.086\n",
      "[Epoch 4, Batch 201 of 310] this loss: 0.158, total loss: 0.085\n",
      "[Epoch 4, Batch 206 of 310] this loss: 0.171, total loss: 0.086\n",
      "[Epoch 4, Batch 211 of 310] this loss: 0.126, total loss: 0.086\n",
      "[Epoch 4, Batch 216 of 310] this loss: 0.114, total loss: 0.086\n",
      "[Epoch 4, Batch 221 of 310] this loss: 0.130, total loss: 0.086\n",
      "[Epoch 4, Batch 226 of 310] this loss: 0.156, total loss: 0.086\n",
      "[Epoch 4, Batch 231 of 310] this loss: 0.152, total loss: 0.086\n",
      "[Epoch 4, Batch 236 of 310] this loss: 0.141, total loss: 0.085\n",
      "[Epoch 4, Batch 241 of 310] this loss: 0.148, total loss: 0.086\n",
      "[Epoch 4, Batch 246 of 310] this loss: 0.130, total loss: 0.085\n",
      "[Epoch 4, Batch 251 of 310] this loss: 0.136, total loss: 0.085\n",
      "[Epoch 4, Batch 256 of 310] this loss: 0.131, total loss: 0.085\n",
      "[Epoch 4, Batch 261 of 310] this loss: 0.154, total loss: 0.085\n",
      "[Epoch 4, Batch 266 of 310] this loss: 0.141, total loss: 0.085\n",
      "[Epoch 4, Batch 271 of 310] this loss: 0.127, total loss: 0.085\n",
      "[Epoch 4, Batch 276 of 310] this loss: 0.155, total loss: 0.085\n",
      "[Epoch 4, Batch 281 of 310] this loss: 0.133, total loss: 0.085\n",
      "[Epoch 4, Batch 286 of 310] this loss: 0.136, total loss: 0.085\n",
      "[Epoch 4, Batch 291 of 310] this loss: 0.155, total loss: 0.085\n",
      "[Epoch 4, Batch 296 of 310] this loss: 0.162, total loss: 0.085\n",
      "[Epoch 4, Batch 301 of 310] this loss: 0.109, total loss: 0.085\n",
      "[Epoch 4, Batch 306 of 310] this loss: 0.141, total loss: 0.085\n",
      "[Epoch 5, Batch 1 of 310] this loss: 0.027, total loss: 0.085\n",
      "[Epoch 5, Batch 6 of 310] this loss: 0.137, total loss: 0.085\n",
      "[Epoch 5, Batch 11 of 310] this loss: 0.124, total loss: 0.085\n",
      "[Epoch 5, Batch 16 of 310] this loss: 0.170, total loss: 0.085\n",
      "[Epoch 5, Batch 21 of 310] this loss: 0.149, total loss: 0.085\n",
      "[Epoch 5, Batch 26 of 310] this loss: 0.113, total loss: 0.085\n",
      "[Epoch 5, Batch 31 of 310] this loss: 0.147, total loss: 0.085\n",
      "[Epoch 5, Batch 36 of 310] this loss: 0.129, total loss: 0.085\n",
      "[Epoch 5, Batch 41 of 310] this loss: 0.106, total loss: 0.085\n",
      "[Epoch 5, Batch 46 of 310] this loss: 0.144, total loss: 0.085\n",
      "[Epoch 5, Batch 51 of 310] this loss: 0.139, total loss: 0.085\n",
      "[Epoch 5, Batch 56 of 310] this loss: 0.136, total loss: 0.085\n",
      "[Epoch 5, Batch 61 of 310] this loss: 0.130, total loss: 0.085\n",
      "[Epoch 5, Batch 66 of 310] this loss: 0.133, total loss: 0.085\n",
      "[Epoch 5, Batch 71 of 310] this loss: 0.150, total loss: 0.085\n",
      "[Epoch 5, Batch 76 of 310] this loss: 0.169, total loss: 0.085\n",
      "[Epoch 5, Batch 81 of 310] this loss: 0.139, total loss: 0.085\n",
      "[Epoch 5, Batch 86 of 310] this loss: 0.115, total loss: 0.085\n",
      "[Epoch 5, Batch 91 of 310] this loss: 0.173, total loss: 0.085\n",
      "[Epoch 5, Batch 96 of 310] this loss: 0.142, total loss: 0.085\n",
      "[Epoch 5, Batch 101 of 310] this loss: 0.162, total loss: 0.085\n",
      "[Epoch 5, Batch 106 of 310] this loss: 0.119, total loss: 0.085\n",
      "[Epoch 5, Batch 111 of 310] this loss: 0.119, total loss: 0.085\n",
      "[Epoch 5, Batch 116 of 310] this loss: 0.161, total loss: 0.085\n",
      "[Epoch 5, Batch 121 of 310] this loss: 0.152, total loss: 0.085\n",
      "[Epoch 5, Batch 126 of 310] this loss: 0.170, total loss: 0.085\n",
      "[Epoch 5, Batch 131 of 310] this loss: 0.132, total loss: 0.085\n",
      "[Epoch 5, Batch 136 of 310] this loss: 0.128, total loss: 0.085\n",
      "[Epoch 5, Batch 141 of 310] this loss: 0.145, total loss: 0.085\n",
      "[Epoch 5, Batch 146 of 310] this loss: 0.138, total loss: 0.085\n",
      "[Epoch 5, Batch 151 of 310] this loss: 0.130, total loss: 0.085\n",
      "[Epoch 5, Batch 156 of 310] this loss: 0.129, total loss: 0.085\n",
      "[Epoch 5, Batch 161 of 310] this loss: 0.156, total loss: 0.085\n",
      "[Epoch 5, Batch 166 of 310] this loss: 0.136, total loss: 0.085\n",
      "[Epoch 5, Batch 171 of 310] this loss: 0.149, total loss: 0.085\n",
      "[Epoch 5, Batch 176 of 310] this loss: 0.140, total loss: 0.085\n",
      "[Epoch 5, Batch 181 of 310] this loss: 0.112, total loss: 0.085\n",
      "[Epoch 5, Batch 186 of 310] this loss: 0.147, total loss: 0.085\n",
      "[Epoch 5, Batch 191 of 310] this loss: 0.128, total loss: 0.085\n",
      "[Epoch 5, Batch 196 of 310] this loss: 0.160, total loss: 0.085\n",
      "[Epoch 5, Batch 201 of 310] this loss: 0.126, total loss: 0.085\n",
      "[Epoch 5, Batch 206 of 310] this loss: 0.115, total loss: 0.085\n",
      "[Epoch 5, Batch 211 of 310] this loss: 0.111, total loss: 0.085\n",
      "[Epoch 5, Batch 216 of 310] this loss: 0.147, total loss: 0.085\n",
      "[Epoch 5, Batch 221 of 310] this loss: 0.129, total loss: 0.085\n",
      "[Epoch 5, Batch 226 of 310] this loss: 0.162, total loss: 0.085\n",
      "[Epoch 5, Batch 231 of 310] this loss: 0.146, total loss: 0.085\n",
      "[Epoch 5, Batch 236 of 310] this loss: 0.116, total loss: 0.085\n",
      "[Epoch 5, Batch 241 of 310] this loss: 0.111, total loss: 0.085\n",
      "[Epoch 5, Batch 246 of 310] this loss: 0.157, total loss: 0.085\n",
      "[Epoch 5, Batch 251 of 310] this loss: 0.134, total loss: 0.085\n",
      "[Epoch 5, Batch 256 of 310] this loss: 0.141, total loss: 0.085\n",
      "[Epoch 5, Batch 261 of 310] this loss: 0.142, total loss: 0.085\n",
      "[Epoch 5, Batch 266 of 310] this loss: 0.144, total loss: 0.085\n",
      "[Epoch 5, Batch 271 of 310] this loss: 0.145, total loss: 0.085\n",
      "[Epoch 5, Batch 276 of 310] this loss: 0.144, total loss: 0.085\n",
      "[Epoch 5, Batch 281 of 310] this loss: 0.142, total loss: 0.085\n",
      "[Epoch 5, Batch 286 of 310] this loss: 0.148, total loss: 0.085\n",
      "[Epoch 5, Batch 291 of 310] this loss: 0.139, total loss: 0.085\n",
      "[Epoch 5, Batch 296 of 310] this loss: 0.154, total loss: 0.085\n",
      "[Epoch 5, Batch 301 of 310] this loss: 0.137, total loss: 0.085\n",
      "[Epoch 5, Batch 306 of 310] this loss: 0.132, total loss: 0.085\n",
      "[Epoch 6, Batch 1 of 310] this loss: 0.018, total loss: 0.085\n",
      "[Epoch 6, Batch 6 of 310] this loss: 0.144, total loss: 0.085\n",
      "[Epoch 6, Batch 11 of 310] this loss: 0.145, total loss: 0.085\n",
      "[Epoch 6, Batch 16 of 310] this loss: 0.135, total loss: 0.085\n",
      "[Epoch 6, Batch 21 of 310] this loss: 0.152, total loss: 0.085\n",
      "[Epoch 6, Batch 26 of 310] this loss: 0.158, total loss: 0.085\n",
      "[Epoch 6, Batch 31 of 310] this loss: 0.157, total loss: 0.085\n",
      "[Epoch 6, Batch 36 of 310] this loss: 0.141, total loss: 0.085\n",
      "[Epoch 6, Batch 41 of 310] this loss: 0.137, total loss: 0.085\n",
      "[Epoch 6, Batch 46 of 310] this loss: 0.147, total loss: 0.085\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset and dataloader for MLM training\n",
    "dataset = MLMDataset(documents, tokenizer, max_len=512)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Train the model for MLM\n",
    "total_loss = 0.0\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    this_running_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        masked_ids, attention_mask, segment_ids, input_ids = batch\n",
    "        outputs = model(masked_ids, attention_mask=attention_mask, token_type_ids=segment_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        this_running_loss += loss.item()\n",
    "        total_loss += this_running_loss\n",
    "        if i % 5 == 0:\n",
    "            print('[Epoch %d, Batch %d of %d] this loss: %.3f, total loss: %.3f' % (epoch+1, i+1, len(dataloader), this_running_loss/100, total_loss/(i+1+epoch*len(dataloader))/100))\n",
    "            this_running_loss = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (politics_nlp)",
   "language": "python",
   "name": "politics_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
