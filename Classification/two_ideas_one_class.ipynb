{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_MyModule = '..'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, path_to_MyModule) \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from MyModule.GeneralFunctions import *\n",
    "from MyModule.SummarizationFunctions import *\n",
    "from MyModule.SamplingFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('..\\datos.xlsx')[['ID','texto','desafio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza\n",
    "df.drop_duplicates(subset='texto', inplace=True)\n",
    "\n",
    "# Quitando texto de mas en columna \"desafio\"\n",
    "df['desafio'] = df['desafio'].apply(lambda x: re.findall('[0-9]+', x)[0])\n",
    "\n",
    "# A str\n",
    "df['texto'] = df['texto'].astype(str)\n",
    "\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create target variable: \n",
    "True if the pair comes from the same desafio, false otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples containing all possible pairs of strings and ID's\n",
    "import itertools\n",
    "id_pairs = list(itertools.combinations(df['ID'].values, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target: 1 if both ID's are from the same desafio, 0 otherwise\n",
    "target = []\n",
    "for id1, id2 in id_pairs:\n",
    "    for desafio in df['desafio'].unique():\n",
    "        ids_desafio = df[df['desafio']==desafio]['ID'].values\n",
    "        if id1 in ids_desafio and id2 in ids_desafio:\n",
    "            target.append(1)\n",
    "            break\n",
    "        elif id1 in ids_desafio or id2 in ids_desafio:\n",
    "            target.append(0)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predic = pd.DataFrame([id_pairs, target]).T\n",
    "df_predic.columns = ['id_pairs','target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are this two ideas from the same desafio?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence embedding feature\n",
    "\n",
    "Three feature alternatives for sentence embedding:\n",
    "\n",
    "1. A single column (tuples of (1, 768) dimensions)\n",
    "2. One column for each of the 768 D\n",
    "3. Reduce dimension with PCA or Lasso/Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('hiiamsid/sentence_similarity_spanish_es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_documents = df['texto'].values\n",
    "\n",
    "pp_object = Preprocess(lemma=False, stopwords=False, alphanumeric=False, join=False)\n",
    "\n",
    "pp_documents = pp_object.preprocess(original_documents)\n",
    "\n",
    "emb_docs = model.encode(pp_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as featrues the document vectors for each document in the pair\n",
    "\n",
    "first_doc_emb = []\n",
    "second_doc_emb = []\n",
    "\n",
    "for pair1, pair2 in df_predic['id_pairs'].values:\n",
    "    \n",
    "    indice_pair1 = df[df['ID']==pair1].index[0]\n",
    "    first_doc_emb.append(emb_docs[indice_pair1])\n",
    "    \n",
    "    indice_pair2 = df[df['ID']==pair2].index[0]\n",
    "    second_doc_emb.append(emb_docs[indice_pair2])\n",
    "    \n",
    "\n",
    "df_predic['first_doc_emb'] = first_doc_emb\n",
    "df_predic['second_doc_emb'] = second_doc_emb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predic = pd.read_excel('embedding_df.xlsx')\n",
    "\n",
    "df_predic['id_pairs'] = df_predic['id_pairs'].apply(lambda x: string_to_tuple(x)) # recovering tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Felipe/.cache\\huggingface\\hub\\models--pysentimiento--robertuito-sentiment-analysis\\snapshots\\e3be95c8efad7f480ce8aab2221188ecb78e40f3\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-sentiment-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Felipe/.cache\\huggingface\\hub\\models--pysentimiento--robertuito-sentiment-analysis\\snapshots\\e3be95c8efad7f480ce8aab2221188ecb78e40f3\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/robertuito-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "loading file tokenizer.json from cache at C:\\Users\\Felipe/.cache\\huggingface\\hub\\models--pysentimiento--robertuito-sentiment-analysis\\snapshots\\e3be95c8efad7f480ce8aab2221188ecb78e40f3\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\Felipe/.cache\\huggingface\\hub\\models--pysentimiento--robertuito-sentiment-analysis\\snapshots\\e3be95c8efad7f480ce8aab2221188ecb78e40f3\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Felipe/.cache\\huggingface\\hub\\models--pysentimiento--robertuito-sentiment-analysis\\snapshots\\e3be95c8efad7f480ce8aab2221188ecb78e40f3\\tokenizer_config.json\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing 0 of 505\n",
      "Doing 1 of 505\n",
      "Doing 2 of 505\n",
      "Doing 3 of 505\n",
      "Doing 4 of 505\n",
      "Doing 5 of 505\n",
      "Doing 6 of 505\n",
      "Doing 7 of 505\n",
      "Doing 8 of 505\n",
      "Doing 9 of 505\n",
      "Doing 10 of 505\n",
      "Doing 11 of 505\n",
      "Doing 12 of 505\n",
      "Doing 13 of 505\n",
      "Doing 14 of 505\n",
      "Doing 15 of 505\n",
      "Doing 16 of 505\n",
      "Doing 17 of 505\n",
      "Doing 18 of 505\n",
      "Doing 19 of 505\n",
      "Doing 20 of 505\n",
      "Doing 21 of 505\n",
      "Doing 22 of 505\n",
      "Doing 23 of 505\n",
      "Doing 24 of 505\n",
      "Doing 25 of 505\n",
      "Doing 26 of 505\n",
      "Doing 27 of 505\n",
      "Doing 28 of 505\n",
      "Doing 29 of 505\n",
      "Doing 30 of 505\n",
      "Doing 31 of 505\n",
      "Doing 32 of 505\n",
      "Doing 33 of 505\n",
      "Doing 34 of 505\n",
      "Doing 35 of 505\n",
      "Doing 36 of 505\n",
      "Doing 37 of 505\n",
      "Doing 38 of 505\n",
      "Doing 39 of 505\n",
      "Doing 40 of 505\n",
      "Doing 41 of 505\n",
      "Doing 42 of 505\n",
      "Doing 43 of 505\n",
      "Doing 44 of 505\n",
      "Doing 45 of 505\n",
      "Doing 46 of 505\n",
      "Doing 47 of 505\n",
      "Doing 48 of 505\n",
      "Doing 49 of 505\n",
      "Doing 50 of 505\n",
      "Doing 51 of 505\n",
      "Doing 52 of 505\n",
      "Doing 53 of 505\n",
      "Doing 54 of 505\n",
      "Doing 55 of 505\n",
      "Doing 56 of 505\n",
      "Doing 57 of 505\n",
      "Doing 58 of 505\n",
      "Doing 59 of 505\n",
      "Doing 60 of 505\n",
      "Doing 61 of 505\n",
      "Doing 62 of 505\n",
      "Doing 63 of 505\n",
      "Doing 64 of 505\n",
      "Doing 65 of 505\n",
      "Doing 66 of 505\n",
      "Doing 67 of 505\n",
      "Doing 68 of 505\n",
      "Doing 69 of 505\n",
      "Doing 70 of 505\n",
      "Doing 71 of 505\n",
      "Doing 72 of 505\n",
      "Doing 73 of 505\n",
      "Doing 74 of 505\n",
      "Doing 75 of 505\n",
      "Doing 76 of 505\n",
      "Doing 77 of 505\n",
      "Doing 78 of 505\n",
      "Doing 79 of 505\n",
      "Doing 80 of 505\n",
      "Doing 81 of 505\n",
      "Doing 82 of 505\n",
      "Doing 83 of 505\n",
      "Doing 84 of 505\n",
      "Doing 85 of 505\n",
      "Doing 86 of 505\n",
      "Doing 87 of 505\n",
      "Doing 88 of 505\n",
      "Doing 89 of 505\n",
      "Doing 90 of 505\n",
      "Doing 91 of 505\n",
      "Doing 92 of 505\n",
      "Doing 93 of 505\n",
      "Doing 94 of 505\n",
      "Doing 95 of 505\n",
      "Doing 96 of 505\n",
      "Doing 97 of 505\n",
      "Doing 98 of 505\n",
      "Doing 99 of 505\n",
      "Doing 100 of 505\n",
      "Doing 101 of 505\n",
      "Doing 102 of 505\n",
      "Doing 103 of 505\n",
      "Doing 104 of 505\n",
      "Doing 105 of 505\n",
      "Doing 106 of 505\n",
      "Doing 107 of 505\n",
      "Doing 108 of 505\n",
      "Doing 109 of 505\n",
      "Doing 110 of 505\n",
      "Doing 111 of 505\n",
      "Doing 112 of 505\n",
      "Doing 113 of 505\n",
      "Doing 114 of 505\n",
      "Doing 115 of 505\n",
      "Doing 116 of 505\n",
      "Doing 117 of 505\n",
      "Doing 118 of 505\n",
      "Doing 119 of 505\n",
      "Doing 120 of 505\n",
      "Doing 121 of 505\n",
      "Doing 122 of 505\n",
      "Doing 123 of 505\n",
      "Doing 124 of 505\n",
      "Doing 125 of 505\n",
      "Doing 126 of 505\n",
      "Doing 127 of 505\n",
      "Doing 128 of 505\n",
      "Doing 129 of 505\n",
      "Doing 130 of 505\n",
      "Doing 131 of 505\n",
      "Doing 132 of 505\n",
      "Doing 133 of 505\n",
      "Doing 134 of 505\n",
      "Doing 135 of 505\n",
      "Doing 136 of 505\n",
      "Doing 137 of 505\n",
      "Doing 138 of 505\n",
      "Doing 139 of 505\n",
      "Doing 140 of 505\n",
      "Doing 141 of 505\n",
      "Doing 142 of 505\n",
      "Doing 143 of 505\n",
      "Doing 144 of 505\n",
      "Doing 145 of 505\n",
      "Doing 146 of 505\n",
      "Doing 147 of 505\n",
      "Doing 148 of 505\n",
      "Doing 149 of 505\n",
      "Doing 150 of 505\n",
      "Doing 151 of 505\n",
      "Doing 152 of 505\n",
      "Doing 153 of 505\n",
      "Doing 154 of 505\n",
      "Doing 155 of 505\n",
      "Doing 156 of 505\n",
      "Doing 157 of 505\n",
      "Doing 158 of 505\n",
      "Doing 159 of 505\n",
      "Doing 160 of 505\n",
      "Doing 161 of 505\n",
      "Doing 162 of 505\n",
      "Doing 163 of 505\n",
      "Doing 164 of 505\n",
      "Doing 165 of 505\n",
      "Doing 166 of 505\n",
      "Doing 167 of 505\n",
      "Doing 168 of 505\n",
      "Doing 169 of 505\n",
      "Doing 170 of 505\n",
      "Doing 171 of 505\n",
      "Doing 172 of 505\n",
      "Doing 173 of 505\n",
      "Doing 174 of 505\n",
      "Doing 175 of 505\n",
      "Doing 176 of 505\n",
      "Doing 177 of 505\n",
      "Doing 178 of 505\n",
      "Doing 179 of 505\n",
      "Doing 180 of 505\n",
      "Doing 181 of 505\n",
      "Doing 182 of 505\n",
      "Doing 183 of 505\n",
      "Doing 184 of 505\n",
      "Doing 185 of 505\n",
      "Doing 186 of 505\n",
      "Doing 187 of 505\n",
      "Doing 188 of 505\n",
      "Doing 189 of 505\n",
      "Doing 190 of 505\n",
      "Doing 191 of 505\n",
      "Doing 192 of 505\n",
      "Doing 193 of 505\n",
      "Doing 194 of 505\n",
      "Doing 195 of 505\n",
      "Doing 196 of 505\n",
      "Doing 197 of 505\n",
      "Doing 198 of 505\n",
      "Doing 199 of 505\n",
      "Doing 200 of 505\n",
      "Doing 201 of 505\n",
      "Doing 202 of 505\n",
      "Doing 203 of 505\n",
      "Doing 204 of 505\n",
      "Doing 205 of 505\n",
      "Doing 206 of 505\n",
      "Doing 207 of 505\n",
      "Doing 208 of 505\n",
      "Doing 209 of 505\n",
      "Doing 210 of 505\n",
      "Doing 211 of 505\n",
      "Doing 212 of 505\n",
      "Doing 213 of 505\n",
      "Doing 214 of 505\n",
      "Doing 215 of 505\n",
      "Doing 216 of 505\n",
      "Doing 217 of 505\n",
      "Doing 218 of 505\n",
      "Doing 219 of 505\n",
      "Doing 220 of 505\n",
      "Doing 221 of 505\n",
      "Doing 222 of 505\n",
      "Doing 223 of 505\n",
      "Doing 224 of 505\n",
      "Doing 225 of 505\n",
      "Doing 226 of 505\n",
      "Doing 227 of 505\n",
      "Doing 228 of 505\n",
      "Doing 229 of 505\n",
      "Doing 230 of 505\n",
      "Doing 231 of 505\n",
      "Doing 232 of 505\n",
      "Doing 233 of 505\n",
      "Doing 234 of 505\n",
      "Doing 235 of 505\n",
      "Doing 236 of 505\n",
      "Doing 237 of 505\n",
      "Doing 238 of 505\n",
      "Doing 239 of 505\n",
      "Doing 240 of 505\n",
      "Doing 241 of 505\n",
      "Doing 242 of 505\n",
      "Doing 243 of 505\n",
      "Doing 244 of 505\n",
      "Doing 245 of 505\n",
      "Doing 246 of 505\n",
      "Doing 247 of 505\n",
      "Doing 248 of 505\n",
      "Doing 249 of 505\n",
      "Doing 250 of 505\n",
      "Doing 251 of 505\n",
      "Doing 252 of 505\n",
      "Doing 253 of 505\n",
      "Doing 254 of 505\n",
      "Doing 255 of 505\n",
      "Doing 256 of 505\n",
      "Doing 257 of 505\n",
      "Doing 258 of 505\n",
      "Doing 259 of 505\n",
      "Doing 260 of 505\n",
      "Doing 261 of 505\n",
      "Doing 262 of 505\n",
      "Doing 263 of 505\n",
      "Doing 264 of 505\n",
      "Doing 265 of 505\n",
      "Doing 266 of 505\n",
      "Doing 267 of 505\n",
      "Doing 268 of 505\n",
      "Doing 269 of 505\n",
      "Doing 270 of 505\n",
      "Doing 271 of 505\n",
      "Doing 272 of 505\n",
      "Doing 273 of 505\n",
      "Doing 274 of 505\n",
      "Doing 275 of 505\n",
      "Doing 276 of 505\n",
      "Doing 277 of 505\n",
      "Doing 278 of 505\n",
      "Doing 279 of 505\n",
      "Doing 280 of 505\n",
      "Doing 281 of 505\n",
      "Doing 282 of 505\n",
      "Doing 283 of 505\n",
      "Doing 284 of 505\n",
      "Doing 285 of 505\n",
      "Doing 286 of 505\n",
      "Doing 287 of 505\n",
      "Doing 288 of 505\n",
      "Doing 289 of 505\n",
      "Doing 290 of 505\n",
      "Doing 291 of 505\n",
      "Doing 292 of 505\n",
      "Doing 293 of 505\n",
      "Doing 294 of 505\n",
      "Doing 295 of 505\n",
      "Doing 296 of 505\n",
      "Doing 297 of 505\n",
      "Doing 298 of 505\n",
      "Doing 299 of 505\n",
      "Doing 300 of 505\n",
      "Doing 301 of 505\n",
      "Doing 302 of 505\n",
      "Doing 303 of 505\n",
      "Doing 304 of 505\n",
      "Doing 305 of 505\n",
      "Doing 306 of 505\n",
      "Doing 307 of 505\n",
      "Doing 308 of 505\n",
      "Doing 309 of 505\n",
      "Doing 310 of 505\n",
      "Doing 311 of 505\n",
      "Doing 312 of 505\n",
      "Doing 313 of 505\n",
      "Doing 314 of 505\n",
      "Doing 315 of 505\n",
      "Doing 316 of 505\n",
      "Doing 317 of 505\n",
      "Doing 318 of 505\n",
      "Doing 319 of 505\n",
      "Doing 320 of 505\n",
      "Doing 321 of 505\n",
      "Doing 322 of 505\n",
      "Doing 323 of 505\n",
      "Doing 324 of 505\n",
      "Doing 325 of 505\n",
      "Doing 326 of 505\n",
      "Doing 327 of 505\n",
      "Doing 328 of 505\n",
      "Doing 329 of 505\n",
      "Doing 330 of 505\n",
      "Doing 331 of 505\n",
      "Doing 332 of 505\n",
      "Doing 333 of 505\n",
      "Doing 334 of 505\n",
      "Doing 335 of 505\n",
      "Doing 336 of 505\n",
      "Doing 337 of 505\n",
      "Doing 338 of 505\n",
      "Doing 339 of 505\n",
      "Doing 340 of 505\n",
      "Doing 341 of 505\n",
      "Doing 342 of 505\n",
      "Doing 343 of 505\n",
      "Doing 344 of 505\n",
      "Doing 345 of 505\n",
      "Doing 346 of 505\n",
      "Doing 347 of 505\n",
      "Doing 348 of 505\n",
      "Doing 349 of 505\n",
      "Doing 350 of 505\n",
      "Doing 351 of 505\n",
      "Doing 352 of 505\n",
      "Doing 353 of 505\n",
      "Doing 354 of 505\n",
      "Doing 355 of 505\n",
      "Doing 356 of 505\n",
      "Doing 357 of 505\n",
      "Doing 358 of 505\n",
      "Doing 359 of 505\n",
      "Doing 360 of 505\n",
      "Doing 361 of 505\n",
      "Doing 362 of 505\n",
      "Doing 363 of 505\n",
      "Doing 364 of 505\n",
      "Doing 365 of 505\n",
      "Doing 366 of 505\n",
      "Doing 367 of 505\n",
      "Doing 368 of 505\n",
      "Doing 369 of 505\n",
      "Doing 370 of 505\n",
      "Doing 371 of 505\n",
      "Doing 372 of 505\n",
      "Doing 373 of 505\n",
      "Doing 374 of 505\n",
      "Doing 375 of 505\n",
      "Doing 376 of 505\n",
      "Doing 377 of 505\n",
      "Doing 378 of 505\n",
      "Doing 379 of 505\n",
      "Doing 380 of 505\n",
      "Doing 381 of 505\n",
      "Doing 382 of 505\n",
      "Doing 383 of 505\n",
      "Doing 384 of 505\n",
      "Doing 385 of 505\n",
      "Doing 386 of 505\n",
      "Doing 387 of 505\n",
      "Doing 388 of 505\n",
      "Doing 389 of 505\n",
      "Doing 390 of 505\n",
      "Doing 391 of 505\n",
      "Doing 392 of 505\n",
      "Doing 393 of 505\n",
      "Doing 394 of 505\n",
      "Doing 395 of 505\n",
      "Doing 396 of 505\n",
      "Doing 397 of 505\n",
      "Doing 398 of 505\n",
      "Doing 399 of 505\n",
      "Doing 400 of 505\n",
      "Doing 401 of 505\n",
      "Doing 402 of 505\n",
      "Doing 403 of 505\n",
      "Doing 404 of 505\n",
      "Doing 405 of 505\n",
      "Doing 406 of 505\n",
      "Doing 407 of 505\n",
      "Doing 408 of 505\n",
      "Doing 409 of 505\n",
      "Doing 410 of 505\n",
      "Doing 411 of 505\n",
      "Doing 412 of 505\n",
      "Doing 413 of 505\n",
      "Doing 414 of 505\n",
      "Doing 415 of 505\n",
      "Doing 416 of 505\n",
      "Doing 417 of 505\n",
      "Doing 418 of 505\n",
      "Doing 419 of 505\n",
      "Doing 420 of 505\n",
      "Doing 421 of 505\n",
      "Doing 422 of 505\n",
      "Doing 423 of 505\n",
      "Doing 424 of 505\n",
      "Doing 425 of 505\n",
      "Doing 426 of 505\n",
      "Doing 427 of 505\n",
      "Doing 428 of 505\n",
      "Doing 429 of 505\n",
      "Doing 430 of 505\n",
      "Doing 431 of 505\n",
      "Doing 432 of 505\n",
      "Doing 433 of 505\n",
      "Doing 434 of 505\n",
      "Doing 435 of 505\n",
      "Doing 436 of 505\n",
      "Doing 437 of 505\n",
      "Doing 438 of 505\n",
      "Doing 439 of 505\n",
      "Doing 440 of 505\n",
      "Doing 441 of 505\n",
      "Doing 442 of 505\n",
      "Doing 443 of 505\n",
      "Doing 444 of 505\n",
      "Doing 445 of 505\n",
      "Doing 446 of 505\n",
      "Doing 447 of 505\n",
      "Doing 448 of 505\n",
      "Doing 449 of 505\n",
      "Doing 450 of 505\n",
      "Doing 451 of 505\n",
      "Doing 452 of 505\n",
      "Doing 453 of 505\n",
      "Doing 454 of 505\n",
      "Doing 455 of 505\n",
      "Doing 456 of 505\n",
      "Doing 457 of 505\n",
      "Doing 458 of 505\n",
      "Doing 459 of 505\n",
      "Doing 460 of 505\n",
      "Doing 461 of 505\n",
      "Doing 462 of 505\n",
      "Doing 463 of 505\n",
      "Doing 464 of 505\n",
      "Doing 465 of 505\n",
      "Doing 466 of 505\n",
      "Doing 467 of 505\n",
      "Doing 468 of 505\n",
      "Doing 469 of 505\n",
      "Doing 470 of 505\n",
      "Doing 471 of 505\n",
      "Doing 472 of 505\n",
      "Doing 473 of 505\n",
      "Doing 474 of 505\n",
      "Doing 475 of 505\n",
      "Doing 476 of 505\n",
      "Doing 477 of 505\n",
      "Doing 478 of 505\n",
      "Doing 479 of 505\n",
      "Doing 480 of 505\n",
      "Doing 481 of 505\n",
      "Doing 482 of 505\n",
      "Doing 483 of 505\n",
      "Doing 484 of 505\n",
      "Doing 485 of 505\n",
      "Doing 486 of 505\n",
      "Doing 487 of 505\n",
      "Doing 488 of 505\n",
      "Doing 489 of 505\n",
      "Doing 490 of 505\n",
      "Doing 491 of 505\n",
      "Doing 492 of 505\n",
      "Doing 493 of 505\n",
      "Doing 494 of 505\n",
      "Doing 495 of 505\n",
      "Doing 496 of 505\n",
      "Doing 497 of 505\n",
      "Doing 498 of 505\n",
      "Doing 499 of 505\n",
      "Doing 500 of 505\n",
      "Doing 501 of 505\n",
      "Doing 502 of 505\n",
      "Doing 503 of 505\n",
      "Doing 504 of 505\n"
     ]
    }
   ],
   "source": [
    "# Analyze the three sentiments for each doc\n",
    "from MyModule.SentimentAnalysisFunctions import sentiment_analyzer_3d\n",
    "\n",
    "all_emotions = {}\n",
    "analyzer = sentiment_analyzer_3d()\n",
    "\n",
    "for i, this_id in enumerate(df['ID'].values):\n",
    "    print('Doing {} of {}'.format(i, len(df['ID'].values)))\n",
    "    all_emotions[this_id] = analyzer.predict_sentiment_3d(pp_documents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as featrues the document vectors for each document in the pair\n",
    "\n",
    "first_doc_pos = []\n",
    "first_doc_neg = []\n",
    "first_doc_neu = []\n",
    "\n",
    "second_doc_pos = []\n",
    "second_doc_neg = []\n",
    "second_doc_neu = []\n",
    "\n",
    "for pair1, pair2 in df_predic['id_pairs'].values:\n",
    "    \n",
    "    first_doc_pos.append(all_emotions[pair1][0])\n",
    "    first_doc_neg.append(all_emotions[pair1][1])\n",
    "    first_doc_neu.append(all_emotions[pair1][2])\n",
    "    \n",
    "    second_doc_pos.append(all_emotions[pair2][0])\n",
    "    second_doc_neg.append(all_emotions[pair2][1])\n",
    "    second_doc_neu.append(all_emotions[pair2][2])\n",
    "    \n",
    "\n",
    "df_predic['first_doc_pos'] = first_doc_pos\n",
    "df_predic['first_doc_neg'] = first_doc_neg\n",
    "df_predic['first_doc_neu'] = first_doc_neu\n",
    "\n",
    "df_predic['second_doc_pos'] = second_doc_pos\n",
    "df_predic['second_doc_neg'] = second_doc_neg\n",
    "df_predic['second_doc_neu'] = second_doc_neu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of each documents\n",
    "\n",
    "df_predic = pd.read_excel('embedding_sent_df.xlsx')\n",
    "\n",
    "df_predic['id_pairs'] = df_predic['id_pairs'].apply(lambda x: string_to_tuple(x)) # recovering tuples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (politics_nlp)",
   "language": "python",
   "name": "politics_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89ff38eb85f6997b5e3c2063f2dbb85e394a03e8d4e24c64f841e981b030dc0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
